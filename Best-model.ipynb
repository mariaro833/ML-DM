{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e77a6d6-2c91-450c-88ca-a0744b2c6395",
   "metadata": {},
   "source": [
    "* Number of records: 214\n",
    "* Number of features: 9 + 1 (target feature)\n",
    "* Repository URL: https://archive.ics.uci.edu/dataset/42/glass+identification\n",
    " \n",
    "##### Problems:\n",
    "a. Find the best two models by creating a complete pipeline per each model, that explores both models and parameters. Comment and compare the results.\\\n",
    "b. Benchmark the best two models in __a.__ by different cross-validation techniques (at least 3). Comment results.\\\n",
    "c. Run one AutoML calculation on the dataset. How do these results compare with the obtained in __a.__? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1af7de-353b-411c-bee2-30311e87bb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50a64aa5-47ce-4495-a513-6387f6caf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e809a-5700-479f-bcf0-836a99c7cc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axes(0.125,0.11;0.62x0.77)\n",
      "\n",
      "1. DATASET EXPLORATION\n",
      "--------------------------------------------------------------------------------\n",
      "Dataset shape: (214, 9)\n",
      "\n",
      "Features:\n",
      "['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe']\n",
      "\n",
      "Target variable:\n",
      "['Type_of_glass']\n",
      "\n",
      "\n",
      "Class distribution:\n",
      "Type_of_glass\n",
      "2                76\n",
      "1                70\n",
      "7                29\n",
      "3                17\n",
      "5                13\n",
      "6                 9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Feature statistics:\n",
      "               RI          Na          Mg          Al          Si           K  \\\n",
      "count  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \n",
      "mean     1.518365   13.407850    2.684533    1.444907   72.650935    0.497056   \n",
      "std      0.003037    0.816604    1.442408    0.499270    0.774546    0.652192   \n",
      "min      1.511150   10.730000    0.000000    0.290000   69.810000    0.000000   \n",
      "25%      1.516522   12.907500    2.115000    1.190000   72.280000    0.122500   \n",
      "50%      1.517680   13.300000    3.480000    1.360000   72.790000    0.555000   \n",
      "75%      1.519157   13.825000    3.600000    1.630000   73.087500    0.610000   \n",
      "max      1.533930   17.380000    4.490000    3.500000   75.410000    6.210000   \n",
      "\n",
      "               Ca          Ba          Fe  \n",
      "count  214.000000  214.000000  214.000000  \n",
      "mean     8.956963    0.175047    0.057009  \n",
      "std      1.423153    0.497219    0.097439  \n",
      "min      5.430000    0.000000    0.000000  \n",
      "25%      8.240000    0.000000    0.000000  \n",
      "50%      8.600000    0.000000    0.000000  \n",
      "75%      9.172500    0.000000    0.100000  \n",
      "max     16.190000    3.150000    0.510000  \n",
      "\n",
      "Missing values:\n",
      "RI    0\n",
      "Na    0\n",
      "Mg    0\n",
      "Al    0\n",
      "Si    0\n",
      "K     0\n",
      "Ca    0\n",
      "Ba    0\n",
      "Fe    0\n",
      "dtype: int64\n",
      "\n",
      "Training set size: 171\n",
      "Test set size: 43\n",
      "\n",
      "Hyperparameter grid:\n",
      "  rf__n_estimators: [50, 100, 300]\n",
      "  rf__max_depth: [None, 10, 30]\n",
      "  rf__min_samples_split: [2, 5]\n",
      "  rf__min_samples_leaf: [1, 2, 4]\n",
      "  rf__max_features: ['sqrt', 'log2']\n",
      "\n",
      "Performing GridSearchCV (this may take a moment)...\n",
      "2. MODEL 1: RANDOM FOREST CLASSIFIER\n",
      "\n",
      "Hyperparameter grid:\n",
      "  rf__n_estimators: [50, 100, 200, 300]\n",
      "  rf__max_depth: [None, 10, 20, 30]\n",
      "  rf__min_samples_split: [2, 5, 10]\n",
      "  rf__min_samples_leaf: [1, 2, 4]\n",
      "  rf__max_features: ['sqrt', 'log2']\n",
      "\n",
      "Performing GridSearchCV (this may take a moment)...\n"
     ]
    }
   ],
   "source": [
    "glass_identification = fetch_ucirepo(id=42) \n",
    "X = glass_identification.data.features \n",
    "y = glass_identification.data.targets\n",
    "\n",
    "print(sns.heatmap(X.corr().round(2), annot=True, cmap='coolwarm', center=0))\n",
    "\n",
    "print(\"\\n1. DATASET EXPLORATION\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"\\nFeatures:\\n{X.columns.tolist()}\")\n",
    "print(f\"\\nTarget variable:\\n{y.columns.tolist()}\\n\")\n",
    "print(f\"\\nClass distribution:\\n{y.value_counts()}\")\n",
    "print(f\"\\nFeature statistics:\\n{X.describe()}\")\n",
    "print(f\"\\nMissing values:\\n{X.isnull().sum()}\")\n",
    "\n",
    "y = y.values.ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 300],\n",
    "    'rf__max_depth': [None, 10, 30],\n",
    "    'rf__min_samples_split': [2, 5],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"\\nHyperparameter grid:\")\n",
    "for param, values in rf_param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(\"\\nPerforming GridSearchCV (this may take a moment)...\")\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_pipeline, \n",
    "    rf_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"2. MODEL 1: RANDOM FOREST CLASSIFIER\")\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200, 300],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4],\n",
    "    'rf__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(\"\\nHyperparameter grid:\")\n",
    "for param, values in rf_param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "print(\"\\nPerforming GridSearchCV (this may take a moment)...\")\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_pipeline, \n",
    "    rf_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n--- Random Forest Results ---\")\n",
    "print(f\"Best parameters: {rf_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {rf_grid_search.best_score_:.4f}\")\n",
    "\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "y_pred_rf = rf_best_model.predict(X_test)\n",
    "rf_test_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {rf_test_accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "\n",
    "feature_importance_rf = rf_best_model.named_steps['rf'].feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "feature_importance_df_rf = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance_rf\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance (Top 5):\")\n",
    "print(feature_importance_df_rf.head())\n",
    "\n",
    "print(\"3. MODEL 2: GRADIENT BOOSTING CLASSIFIER\")\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "gb_param_grid = {\n",
    "    'gb__n_estimators': [50, 100, 200],\n",
    "    'gb__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gb__max_depth': [3, 5, 7],\n",
    "    'gb__min_samples_split': [2, 5, 10],\n",
    "    'gb__min_samples_leaf': [1, 2, 4],\n",
    "    'gb__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(\"\\nHyperparameter grid:\")\n",
    "for param, values in gb_param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# Perform Grid Search with Cross-Validation\n",
    "print(\"\\nPerforming GridSearchCV (this may take a moment)...\")\n",
    "gb_grid_search = GridSearchCV(\n",
    "    gb_pipeline, \n",
    "    gb_param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "gb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Results for Gradient Boosting\n",
    "print(\"\\n--- Gradient Boosting Results ---\")\n",
    "print(f\"Best parameters: {gb_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {gb_grid_search.best_score_:.4f}\")\n",
    "\n",
    "gb_best_model = gb_grid_search.best_estimator_\n",
    "y_pred_gb = gb_best_model.predict(X_test)\n",
    "gb_test_accuracy = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Accuracy: {gb_test_accuracy:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_gb))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance_gb = gb_best_model.named_steps['gb'].feature_importances_\n",
    "feature_importance_df_gb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importance_gb\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance (Top 5):\")\n",
    "print(feature_importance_df_gb.head())\n",
    "\n",
    "print(\"4. COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosting'],\n",
    "    'Best CV Accuracy': [rf_grid_search.best_score_, gb_grid_search.best_score_],\n",
    "    'Test Accuracy': [rf_test_accuracy, gb_test_accuracy],\n",
    "    'CV-Test Gap': [rf_grid_search.best_score_ - rf_test_accuracy, \n",
    "                    gb_grid_search.best_score_ - gb_test_accuracy]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"DETAILED ANALYSIS AND COMMENTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Determine winner\n",
    "if rf_test_accuracy > gb_test_accuracy:\n",
    "    winner = \"Random Forest\"\n",
    "    winner_accuracy = rf_test_accuracy\n",
    "    diff = rf_test_accuracy - gb_test_accuracy\n",
    "else:\n",
    "    winner = \"Gradient Boosting\"\n",
    "    winner_accuracy = gb_test_accuracy\n",
    "    diff = gb_test_accuracy - rf_test_accuracy\n",
    "\n",
    "print(f\"\\n✓ WINNER: {winner}\")\n",
    "print(f\"  - Test Accuracy: {winner_accuracy:.4f}\")\n",
    "print(f\"  - Advantage: {diff:.4f} ({diff*100:.2f}%) over the other model\")\n",
    "\n",
    "print(f\"\\n1. PERFORMANCE ANALYSIS:\")\n",
    "print(f\"   - Random Forest achieved {rf_test_accuracy:.2%} test accuracy\")\n",
    "print(f\"   - Gradient Boosting achieved {gb_test_accuracy:.2%} test accuracy\")\n",
    "print(f\"   - Both models show {'good' if min(rf_test_accuracy, gb_test_accuracy) > 0.70 else 'moderate'} performance\")\n",
    "\n",
    "print(f\"\\n2. GENERALIZATION:\")\n",
    "rf_gap = rf_grid_search.best_score_ - rf_test_accuracy\n",
    "gb_gap = gb_grid_search.best_score_ - gb_test_accuracy\n",
    "print(f\"   - Random Forest CV-Test gap: {rf_gap:.4f} ({'overfitting' if rf_gap > 0.05 else 'good generalization'})\")\n",
    "print(f\"   - Gradient Boosting CV-Test gap: {gb_gap:.4f} ({'overfitting' if gb_gap > 0.05 else 'good generalization'})\")\n",
    "\n",
    "print(f\"\\n3. MODEL CHARACTERISTICS:\")\n",
    "print(f\"   Random Forest:\")\n",
    "print(f\"   - Ensemble of decision trees with bagging\")\n",
    "print(f\"   - Best params: {rf_grid_search.best_params_}\")\n",
    "print(f\"   - More robust to outliers and less prone to overfitting\")\n",
    "print(f\"   - Faster training with parallel processing\")\n",
    "print(f\"\\n   Gradient Boosting:\")\n",
    "print(f\"   - Sequential ensemble with boosting\")\n",
    "print(f\"   - Best params: {gb_grid_search.best_params_}\")\n",
    "print(f\"   - Better at capturing complex patterns\")\n",
    "print(f\"   - More sensitive to hyperparameters\")\n",
    "\n",
    "print(f\"\\n4. FEATURE IMPORTANCE COMPARISON:\")\n",
    "print(f\"\\n   Top 3 features - Random Forest:\")\n",
    "for idx, row in feature_importance_df_rf.head(3).iterrows():\n",
    "    print(f\"   - {row['Feature']}: {row['Importance']:.4f}\")\n",
    "print(f\"\\n   Top 3 features - Gradient Boosting:\")\n",
    "for idx, row in feature_importance_df_gb.head(3).iterrows():\n",
    "    print(f\"   - {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDATIONS:\")\n",
    "if winner == \"Random Forest\":\n",
    "    print(f\"   ✓ Use Random Forest for deployment\")\n",
    "    print(f\"   - Higher test accuracy\")\n",
    "    print(f\"   - Faster prediction times\")\n",
    "    print(f\"   - More interpretable and stable\")\n",
    "else:\n",
    "    print(f\"   ✓ Use Gradient Boosting for deployment\")\n",
    "    print(f\"   - Higher test accuracy\")\n",
    "    print(f\"   - Better at handling complex relationships\")\n",
    "    print(f\"   - Consider ensemble of both models for production\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
